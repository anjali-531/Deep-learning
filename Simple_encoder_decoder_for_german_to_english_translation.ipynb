{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "encoder decoder for german to english translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p80F8v74aT9B",
        "colab_type": "text"
      },
      "source": [
        "##Encoder Decoder using pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3htPx01acA6",
        "colab_type": "text"
      },
      "source": [
        "Importing libraries and our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viA4qig_QpUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import Multi30k #dataset containing german text and their english translation by experts\n",
        "from torchtext.data import Field, BucketIterator \n",
        "import numpy as np\n",
        "import spacy\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kfOPRhtSmd5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "01b38837-45ff-4839-8cfc-dda6f23d6dad"
      },
      "source": [
        "#downloading the spacy english and german models\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciFc0Db4Smnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading the german and english spacy models\n",
        "spacy_ger = spacy.load(\"de\")\n",
        "spacy_eng = spacy.load(\"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4uyQRlmSmsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizing our text\n",
        "def tokenize_ger(text):\n",
        "\"\"\" inputs german text and outputs tokens based on spacy german model\"\"\"\n",
        "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
        "\n",
        "\n",
        "def tokenize_eng(text):\n",
        "\"\"\" inputs english text and outputs tokens based on spacy english model\"\"\"\n",
        "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz5oRrK6SmwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9f08f594-ac38-4ef3-a0c6-79106243d348"
      },
      "source": [
        "#creating pipelines for preprocessing german and english text. Lowercasing, tokenising, and adding <sos> and <eos> at beginning and end of each stream \n",
        "german = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "english = Field(\n",
        "    tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp-C09PFSm0L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "17b390e4-2847-43d9-8fbb-e2ddf1d16006"
      },
      "source": [
        "#downloading our dataset and splitting it into train, valid and test datasets\n",
        "train_data, valid_data, test_data = Multi30k.splits(\n",
        "    exts=(\".de\", \".en\"), fields=(german, english)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEymFvEtSm-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building our german and english vocab with max_size as 10000 and ignoring those tokens whose frequency is less than 2\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_bpA8zAwLbr",
        "colab_type": "text"
      },
      "source": [
        "Creating our encoder , decoder and seq2seq class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhWPQrPdSnGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encoder class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # sentence is first tokenised and that vector is sent here\n",
        "        # x shape: (seq_length, N) where N is batch size\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (seq_length, N, embedding_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding)\n",
        "        # outputs shape: (seq_length, N, hidden_size)\n",
        "        #in encoder we want the context vector so we return hidden,cell\n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV0DuqMWSnJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
        "    ): #here input size is same as output size. input size is size of english vocab and output is vector which corresponds to values for each word which is in vocab(each node represents the probailityb of that word)\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        #takes the context vector hidden,cell and x-> previous predicted word\n",
        "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "        #we want all these outputs as hidden, cell is input for next time and outputs is what the current word should be \n",
        "        predictions = self.fc(outputs)\n",
        "\n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        #we are predicting one word at a time but we want to predict for the entire target sentence and we add the output of decoder one step at time in seq2seq \n",
        "        predictions = predictions.squeeze(0)\n",
        "\n",
        "        return predictions, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7gmlUOgSnN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(english.vocab)\n",
        "        #we predict one word at a time , we are going to do it for an entire batch and every prediction is a vector of vocab size\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0]\n",
        "        #send the target word by word to decoder\n",
        "        for t in range(1, target_len):\n",
        "\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            # With probability of teacher_force_ratio we take the actual next word\n",
        "            # otherwise we take the word that the Decoder predicted it to be.\n",
        "            # Teacher Forcing is used so that the model gets used to seeing\n",
        "            # similar inputs at training and testing time, if teacher forcing is 1\n",
        "            # then inputs at test time might be completely different than what the\n",
        "            # network is used to.\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm6bkbtQSnDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training hyperparameters\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "# Model hyperparameters\n",
        "load_model = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size_encoder = len(german.vocab)\n",
        "input_size_decoder = len(english.vocab)\n",
        "output_size = len(english.vocab)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024  # Needs to be the same for both RNN's\n",
        "num_layers = 2\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyLN7Cb7SnBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "65792de8-b0de-4191-80b1-b29940bc06b9"
      },
      "source": [
        "#create batches for our data\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.src), #tries to create batches that have similar lengths sentences to optimize number of padding\n",
        "    device=device,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hid1RAlSm8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize encoder\n",
        "encoder_net = Encoder(\n",
        "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
        ").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMqqg7ZeUAdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize decoder\n",
        "decoder_net = Decoder(\n",
        "    input_size_decoder,\n",
        "    decoder_embedding_size,\n",
        "    hidden_size,\n",
        "    output_size,\n",
        "    num_layers,\n",
        "    dec_dropout,\n",
        ").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPadTcpXUgdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e1z9suNwaq5",
        "colab_type": "text"
      },
      "source": [
        "Training our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJj87tjTUj_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#padding\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "#we dont want to pay anything for padding in cost function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6WfoBOqSm68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving the model\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWJXLEwYSm5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading the model\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1CwTt7kUmY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if load_model:\n",
        "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op-RazLiUphb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#a sentence in german. We will see how the translated sentence improves as epochs go . It's google translation is \"a boat with other men is pulled to the shore by a large team of horses\"\n",
        "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeqnT1u_QvXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "\n",
        "    # Load german tokenizer\n",
        "    spacy_ger = spacy.load(\"de\")\n",
        "\n",
        "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "\n",
        "    # Go through each german token and convert to an index\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    # Convert to Tensor\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "    #convert indexes to their corresponding word\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "\n",
        "    # remove start token\n",
        "    return translated_sentence[1:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKAc74b4uBJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "0e124162-ba75-4ecb-d8e8-467f77509e15"
      },
      "source": [
        "#!pip install --upgrade git+https://github.com/pytorch/text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/pytorch/text\n",
            "  Cloning https://github.com/pytorch/text to /tmp/pip-req-build-t9ux5a0k\n",
            "  Running command git clone -q https://github.com/pytorch/text /tmp/pip-req-build-t9ux5a0k\n",
            "  Running command git submodule update --init --recursive -q\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdHrBIMxUSLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "def bleu(data, model, german, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, german, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLUHzmcVU1tX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1abbe8b1-79f2-4d68-dc76-323fec76f256"
      },
      "source": [
        "#training model\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "    #saving model\n",
        "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    translated_sentence = translate_sentence(\n",
        "        model, sentence, german, english, device, max_length=50\n",
        "    )\n",
        "\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        # Get input and targets and get to cuda\n",
        "        inp_data = batch.src.to(device)\n",
        "        target = batch.trg.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        output = model(inp_data, target)\n",
        "\n",
        "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
        "        # way that we have output_words * batch_size that we want to send in into\n",
        "        # our cost function, so we need to do some reshapin. While we're at it\n",
        "        # Let's also remove the start token while we're at it\n",
        "        #first output is start token\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "        # within a healthy range\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'man', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'black', 'and', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
            "[Epoch 2 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'male', 'player', 'with', 'a', '<unk>', 'is', 'is', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
            "[Epoch 3 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'street', 'with', 'with', 'two', 'people', 'are', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<eos>']\n",
            "[Epoch 4 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'men', 'with', 'two', 'men', 'are', 'their', '<unk>', 'to', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
            "[Epoch 5 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'man', 'with', 'two', 'men', '<unk>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<eos>']\n",
            "[Epoch 6 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'men', 'with', 'two', 'men', 'are', 'being', 'pulled', 'by', 'a', 'large', '<unk>', '.', '<eos>']\n",
            "[Epoch 7 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'man', 'with', 'with', 'two', 'men', 'being', 'pulled', 'by', 'a', 'large', 'of', 'a', 'large', '.', '.', '<eos>']\n",
            "[Epoch 8 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'bicyclist', 'with', 'with', 'pulled', 'pulled', 'by', 'a', 'large', 'ford', 'of', 'a', 'large', '.', '.', '<eos>']\n",
            "[Epoch 9 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'wings', '<unk>', 'pulled', 'by', 'a', 'large', 'bull', 'by', 'a', 'large', '.', '.', '<eos>']\n",
            "[Epoch 10 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', '<unk>', 'pulled', 'by', 'a', 'few', 'ford', 'by', 'a', 'large', '.', '.', '<eos>']\n",
            "[Epoch 11 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'by', 'a', 'large', 'ford', 'of', 'a', 'large', '<unk>', '.', '<eos>']\n",
            "[Epoch 12 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'bags', 'pulled', 'by', 'a', 'large', 'ford', 'ford', 'by', 'a', 'large', 'river', '.', '<eos>']\n",
            "[Epoch 13 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'pulled', 'by', 'a', 'large', 'cable', 'deere', '.', '<eos>']\n",
            "[Epoch 14 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'pulled', 'a', 'a', 'large', 'by', 'a', 'large', 'river', '.', '<eos>']\n",
            "[Epoch 15 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'pulled', 'by', 'a', 'large', 'gate', 'of', 'a', 'field', '.', '<eos>']\n",
            "[Epoch 16 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', 'horses', '.', '<eos>']\n",
            "[Epoch 17 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'passengers', 'is', 'pulled', 'pulled', 'by', 'a', 'large', 'of', 'of', 'waterfalls', '.', '<eos>']\n",
            "[Epoch 18 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'is', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'crane', '.', '<eos>']\n",
            "[Epoch 19 / 20]\n",
            "=> Saving checkpoint\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'is', 'pulled', 'pulled', 'by', 'a', 'large', 'bull', 'of', 'of', 'a', 'river', 'river', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbDj4nVxU9lF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb95c2d2-8be4-4f43-99f2-a9146582e8c3"
      },
      "source": [
        "score = bleu(test_data[1:100], model, german, english, device)\n",
        "print(f\"Bleu score {score*100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bleu score 17.95\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bi-directional LSTM for sentiment analysis of IMDB reviews dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBWlPMfnesHk",
        "colab_type": "text"
      },
      "source": [
        "##Bi-directional LSTM for sentiment analysis using IMDB dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7cLpLENgXXl",
        "colab_type": "text"
      },
      "source": [
        "Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj0rOnjMerOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch #importing pytorch\n",
        "from torchtext import data #pytorch library for preprocessing\n",
        "import torch.nn.functional as F\n",
        "#setting random seed\n",
        "SEED = 1234\n",
        "# setting manual seed using our random seed to get the same random number\n",
        "torch.manual_seed(SEED)\n",
        "# running on the CuDNN backend\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# tokenizing text using spacy tokenizer and include lengths for packed padded sequences\n",
        "TEXT = data.Field(tokenize = 'spacy',include_lengths=True) \n",
        "# Labelling our dataset and setting tensor to FloatTensors A label field is a shallow wrapper around a standard field designed to hold labels for a classification task\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7bEBxs6gLOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import datasets # to get torch dataset\n",
        "# downloading the IMDb dataset and spliting it into the canonical train/test splits as torchtext.datasets objects\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiEAh9QWiFHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "55a173e2-caf8-4f57-a93f-c35bddbce37d"
      },
      "source": [
        "# Checking length of train test splits\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO12owKd8Vgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e14e7c9-e5c5-436a-e535-b90cf62e2939"
      },
      "source": [
        "train_data[0].text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sydney',\n",
              " 'Lumet',\n",
              " 'has',\n",
              " \"n't\",\n",
              " 'had',\n",
              " 'a',\n",
              " 'box',\n",
              " 'office',\n",
              " 'hit',\n",
              " 'in',\n",
              " '20',\n",
              " 'years',\n",
              " 'and',\n",
              " 'yet',\n",
              " 'at',\n",
              " '83',\n",
              " 'has',\n",
              " 'managed',\n",
              " 'to',\n",
              " 'churn',\n",
              " 'out',\n",
              " 'a',\n",
              " 'tight',\n",
              " ',',\n",
              " 'well',\n",
              " '-',\n",
              " 'cast',\n",
              " ',',\n",
              " 'suspenseful',\n",
              " 'thriller',\n",
              " 'set',\n",
              " 'in',\n",
              " 'his',\n",
              " 'old',\n",
              " 'stamping',\n",
              " 'ground',\n",
              " ',',\n",
              " 'New',\n",
              " 'York',\n",
              " 'City',\n",
              " '.',\n",
              " '(',\n",
              " 'How',\n",
              " 'he',\n",
              " 'got',\n",
              " 'insurance',\n",
              " ',',\n",
              " 'let',\n",
              " 'alone',\n",
              " 'the',\n",
              " 'budget',\n",
              " 'after',\n",
              " 'all',\n",
              " 'those',\n",
              " 'flops',\n",
              " ',',\n",
              " 'is',\n",
              " 'a',\n",
              " 'mystery',\n",
              " 'also',\n",
              " ')',\n",
              " '.',\n",
              " 'The',\n",
              " 'story',\n",
              " 'is',\n",
              " 'a',\n",
              " 'pretty',\n",
              " 'grim',\n",
              " 'one',\n",
              " 'and',\n",
              " 'the',\n",
              " 'characters',\n",
              " 'are',\n",
              " 'not',\n",
              " 'particularly',\n",
              " 'likable',\n",
              " 'but',\n",
              " 'it',\n",
              " 'held',\n",
              " 'me',\n",
              " 'on',\n",
              " 'the',\n",
              " 'edge',\n",
              " 'of',\n",
              " 'my',\n",
              " 'seat',\n",
              " 'till',\n",
              " 'the',\n",
              " 'final',\n",
              " 'scene.<br',\n",
              " '/><br',\n",
              " '/>Two',\n",
              " 'brothers',\n",
              " 'with',\n",
              " 'pressing',\n",
              " 'financial',\n",
              " 'problems',\n",
              " 'conspire',\n",
              " 'to',\n",
              " 'rob',\n",
              " 'a',\n",
              " 'suburban',\n",
              " 'jewelry',\n",
              " 'store',\n",
              " 'owned',\n",
              " 'by',\n",
              " 'their',\n",
              " 'elderly',\n",
              " 'parents',\n",
              " '.',\n",
              " 'The',\n",
              " 'only',\n",
              " 'victim',\n",
              " 'is',\n",
              " 'going',\n",
              " 'to',\n",
              " 'be',\n",
              " 'the',\n",
              " 'insurance',\n",
              " 'company',\n",
              " '.',\n",
              " 'The',\n",
              " 'robbery',\n",
              " 'goes',\n",
              " 'awry',\n",
              " 'and',\n",
              " 'two',\n",
              " 'people',\n",
              " 'die',\n",
              " '.',\n",
              " 'Most',\n",
              " 'of',\n",
              " 'the',\n",
              " 'film',\n",
              " 'is',\n",
              " 'concerned',\n",
              " 'with',\n",
              " 'the',\n",
              " 'aftermath',\n",
              " '.',\n",
              " 'The',\n",
              " 'action',\n",
              " 'is',\n",
              " 'non',\n",
              " '-',\n",
              " 'linear',\n",
              " 'and',\n",
              " 'seen',\n",
              " 'from',\n",
              " 'the',\n",
              " 'main',\n",
              " 'character',\n",
              " \"'s\",\n",
              " 'differing',\n",
              " 'points',\n",
              " 'of',\n",
              " 'view',\n",
              " ',',\n",
              " 'but',\n",
              " 'it',\n",
              " 'is',\n",
              " 'not',\n",
              " 'difficult',\n",
              " 'to',\n",
              " 'follow',\n",
              " '.',\n",
              " 'What',\n",
              " 'is',\n",
              " 'not',\n",
              " 'so',\n",
              " 'easy',\n",
              " 'to',\n",
              " 'work',\n",
              " 'out',\n",
              " 'is',\n",
              " 'the',\n",
              " 'back',\n",
              " 'story',\n",
              " '\\x96',\n",
              " 'how',\n",
              " 'did',\n",
              " 'the',\n",
              " 'brothers',\n",
              " 'get',\n",
              " 'into',\n",
              " 'such',\n",
              " 'a',\n",
              " 'mess',\n",
              " '?',\n",
              " 'There',\n",
              " 'are',\n",
              " 'clues',\n",
              " '\\x96',\n",
              " 'the',\n",
              " 'younger',\n",
              " 'brother',\n",
              " 'being',\n",
              " 'the',\n",
              " 'baby',\n",
              " 'of',\n",
              " 'the',\n",
              " 'family',\n",
              " 'is',\n",
              " 'his',\n",
              " 'fathers',\n",
              " \"'\",\n",
              " 'favorite',\n",
              " 'while',\n",
              " 'the',\n",
              " 'older',\n",
              " 'brother',\n",
              " 'seems',\n",
              " 'to',\n",
              " 'be',\n",
              " 'carrying',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'baggage',\n",
              " 'about',\n",
              " 'his',\n",
              " 'relationship',\n",
              " 'with',\n",
              " 'his',\n",
              " 'father',\n",
              " ',',\n",
              " 'and',\n",
              " 'vice',\n",
              " 'versa',\n",
              " ',',\n",
              " 'but',\n",
              " 'that',\n",
              " 'hardly',\n",
              " 'accounts',\n",
              " 'for',\n",
              " 'him',\n",
              " 'becoming',\n",
              " 'a',\n",
              " 'heroin',\n",
              " '-',\n",
              " 'using',\n",
              " 'murdering',\n",
              " 'embezzler.<br',\n",
              " '/><br',\n",
              " '/>As',\n",
              " 'the',\n",
              " 'scheming',\n",
              " 'older',\n",
              " 'brother',\n",
              " ',',\n",
              " 'a',\n",
              " 'corpulent',\n",
              " 'Philip',\n",
              " 'Seymour',\n",
              " 'Hoffman',\n",
              " 'dominates',\n",
              " 'the',\n",
              " 'film',\n",
              " ',',\n",
              " 'but',\n",
              " 'he',\n",
              " 'is',\n",
              " 'well',\n",
              " 'supported',\n",
              " 'by',\n",
              " 'Ethan',\n",
              " 'Hawke',\n",
              " 'as',\n",
              " 'his',\n",
              " 'bullied',\n",
              " ',',\n",
              " 'inadequate',\n",
              " 'younger',\n",
              " 'brother',\n",
              " '.',\n",
              " 'Albert',\n",
              " 'Finney',\n",
              " 'as',\n",
              " 'their',\n",
              " 'father',\n",
              " 'seems',\n",
              " 'to',\n",
              " 'be',\n",
              " 'in',\n",
              " 'a',\n",
              " 'constant',\n",
              " 'state',\n",
              " 'of',\n",
              " 'rage',\n",
              " 'but',\n",
              " 'then',\n",
              " 'the',\n",
              " 'script',\n",
              " 'calls',\n",
              " 'for',\n",
              " 'that',\n",
              " '.',\n",
              " 'Marisa',\n",
              " 'Tomei',\n",
              " 'as',\n",
              " 'the',\n",
              " 'older',\n",
              " 'brother',\n",
              " \"'s\",\n",
              " 'cheating',\n",
              " 'wife',\n",
              " 'at',\n",
              " 'the',\n",
              " 'age',\n",
              " 'of',\n",
              " '42',\n",
              " 'puts',\n",
              " 'in',\n",
              " 'the',\n",
              " 'sexiest',\n",
              " 'performance',\n",
              " 'I',\n",
              " \"'ve\",\n",
              " 'seen',\n",
              " 'in',\n",
              " 'many',\n",
              " 'a',\n",
              " 'year',\n",
              " '.',\n",
              " 'The',\n",
              " 'film',\n",
              " 'literally',\n",
              " 'starts',\n",
              " 'with',\n",
              " 'a',\n",
              " 'bang',\n",
              " ',',\n",
              " 'but',\n",
              " 'we',\n",
              " 'are',\n",
              " 'out',\n",
              " 'of',\n",
              " 'that',\n",
              " 'comfort',\n",
              " 'zone',\n",
              " 'pretty',\n",
              " 'quickly.<br',\n",
              " '/><br',\n",
              " '/>I',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'know',\n",
              " 'the',\n",
              " 'origins',\n",
              " 'of',\n",
              " 'this',\n",
              " 'story',\n",
              " 'by',\n",
              " 'first',\n",
              " 'time',\n",
              " 'scriptwriter',\n",
              " 'Kelly',\n",
              " 'Masterton',\n",
              " 'but',\n",
              " 'I',\n",
              " 'suspect',\n",
              " 'that',\n",
              " 'like',\n",
              " 'Lumet',\n",
              " \"'s\",\n",
              " 'great',\n",
              " '70',\n",
              " \"'s\",\n",
              " 'film',\n",
              " '\"',\n",
              " 'Dog',\n",
              " 'Day',\n",
              " 'Afternoon',\n",
              " '\"',\n",
              " 'it',\n",
              " 'is',\n",
              " 'based',\n",
              " 'on',\n",
              " 'fact',\n",
              " '\\x96',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'too',\n",
              " 'silly',\n",
              " 'to',\n",
              " 'be',\n",
              " 'untrue',\n",
              " '.',\n",
              " 'Lumet',\n",
              " 'is',\n",
              " 'just',\n",
              " 'about',\n",
              " 'the',\n",
              " 'last',\n",
              " 'of',\n",
              " 'those',\n",
              " 'immensely',\n",
              " 'versatile',\n",
              " 'old',\n",
              " '-',\n",
              " 'time',\n",
              " 'craftsman',\n",
              " 'studio',\n",
              " 'directors',\n",
              " 'who',\n",
              " 'with',\n",
              " 'immense',\n",
              " 'speed',\n",
              " 'were',\n",
              " 'able',\n",
              " 'to',\n",
              " 'direct',\n",
              " 'just',\n",
              " 'about',\n",
              " 'anything',\n",
              " 'that',\n",
              " 'was',\n",
              " 'put',\n",
              " 'in',\n",
              " 'front',\n",
              " 'of',\n",
              " 'them',\n",
              " '.',\n",
              " 'Some',\n",
              " 'great',\n",
              " 'films',\n",
              " 'were',\n",
              " 'produced',\n",
              " 'that',\n",
              " 'way',\n",
              " 'as',\n",
              " 'well',\n",
              " 'as',\n",
              " 'some',\n",
              " 'classic',\n",
              " 'turkeys',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " \"n't\",\n",
              " 'a',\n",
              " 'classic',\n",
              " 'of',\n",
              " 'either',\n",
              " 'sort',\n",
              " '\\x96',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'a',\n",
              " 'well',\n",
              " '-',\n",
              " 'crafted',\n",
              " 'piece',\n",
              " 'of',\n",
              " 'downbeat',\n",
              " 'entertainment',\n",
              " '.',\n",
              " 'It',\n",
              " 'will',\n",
              " 'probably',\n",
              " 'leave',\n",
              " 'you',\n",
              " 'feeling',\n",
              " 'that',\n",
              " 'you',\n",
              " 'were',\n",
              " 'lucky',\n",
              " 'not',\n",
              " 'be',\n",
              " 'a',\n",
              " 'member',\n",
              " 'of',\n",
              " 'a',\n",
              " 'family',\n",
              " 'as',\n",
              " 'dysfunctional',\n",
              " 'as',\n",
              " 'this',\n",
              " 'one',\n",
              " ',',\n",
              " 'but',\n",
              " 'still',\n",
              " 'wondering',\n",
              " 'as',\n",
              " 'to',\n",
              " 'how',\n",
              " 'they',\n",
              " 'got',\n",
              " 'that',\n",
              " 'way',\n",
              " '.',\n",
              " 'We',\n",
              " 'do',\n",
              " 'know',\n",
              " 'the',\n",
              " 'parents',\n",
              " 'were',\n",
              " 'happy',\n",
              " 'but',\n",
              " 'we',\n",
              " 'see',\n",
              " 'so',\n",
              " 'little',\n",
              " 'of',\n",
              " 'the',\n",
              " 'mother',\n",
              " 'and',\n",
              " 'hear',\n",
              " 'so',\n",
              " 'little',\n",
              " 'about',\n",
              " 'her',\n",
              " 'it',\n",
              " 'is',\n",
              " 'impossible',\n",
              " 'pick',\n",
              " 'up',\n",
              " 'on',\n",
              " 'her',\n",
              " 'relationship',\n",
              " 'with',\n",
              " 'the',\n",
              " 'boys',\n",
              " '.',\n",
              " '(',\n",
              " 'There',\n",
              " 'is',\n",
              " 'also',\n",
              " 'a',\n",
              " 'daughter',\n",
              " 'whose',\n",
              " 'presence',\n",
              " 'seems',\n",
              " 'redundant',\n",
              " ')',\n",
              " '.',\n",
              " 'Well',\n",
              " ',',\n",
              " 'like',\n",
              " 'Tolstoy',\n",
              " ',',\n",
              " 'we',\n",
              " 'have',\n",
              " 'to',\n",
              " 'conclude',\n",
              " 'that',\n",
              " '\"',\n",
              " 'each',\n",
              " 'unhappy',\n",
              " 'family',\n",
              " 'is',\n",
              " 'unhappy',\n",
              " 'in',\n",
              " 'its',\n",
              " 'own',\n",
              " 'way',\n",
              " '\"',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6MoXXg-8byQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "558bf062-4aae-4abc-d834-f08851a30864"
      },
      "source": [
        "train_data[0].label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pos'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmA32JD5iOub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aebd7528-b6ba-4962-a97f-7442f05635d5"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['Sydney', 'Lumet', 'has', \"n't\", 'had', 'a', 'box', 'office', 'hit', 'in', '20', 'years', 'and', 'yet', 'at', '83', 'has', 'managed', 'to', 'churn', 'out', 'a', 'tight', ',', 'well', '-', 'cast', ',', 'suspenseful', 'thriller', 'set', 'in', 'his', 'old', 'stamping', 'ground', ',', 'New', 'York', 'City', '.', '(', 'How', 'he', 'got', 'insurance', ',', 'let', 'alone', 'the', 'budget', 'after', 'all', 'those', 'flops', ',', 'is', 'a', 'mystery', 'also', ')', '.', 'The', 'story', 'is', 'a', 'pretty', 'grim', 'one', 'and', 'the', 'characters', 'are', 'not', 'particularly', 'likable', 'but', 'it', 'held', 'me', 'on', 'the', 'edge', 'of', 'my', 'seat', 'till', 'the', 'final', 'scene.<br', '/><br', '/>Two', 'brothers', 'with', 'pressing', 'financial', 'problems', 'conspire', 'to', 'rob', 'a', 'suburban', 'jewelry', 'store', 'owned', 'by', 'their', 'elderly', 'parents', '.', 'The', 'only', 'victim', 'is', 'going', 'to', 'be', 'the', 'insurance', 'company', '.', 'The', 'robbery', 'goes', 'awry', 'and', 'two', 'people', 'die', '.', 'Most', 'of', 'the', 'film', 'is', 'concerned', 'with', 'the', 'aftermath', '.', 'The', 'action', 'is', 'non', '-', 'linear', 'and', 'seen', 'from', 'the', 'main', 'character', \"'s\", 'differing', 'points', 'of', 'view', ',', 'but', 'it', 'is', 'not', 'difficult', 'to', 'follow', '.', 'What', 'is', 'not', 'so', 'easy', 'to', 'work', 'out', 'is', 'the', 'back', 'story', '\\x96', 'how', 'did', 'the', 'brothers', 'get', 'into', 'such', 'a', 'mess', '?', 'There', 'are', 'clues', '\\x96', 'the', 'younger', 'brother', 'being', 'the', 'baby', 'of', 'the', 'family', 'is', 'his', 'fathers', \"'\", 'favorite', 'while', 'the', 'older', 'brother', 'seems', 'to', 'be', 'carrying', 'a', 'lot', 'of', 'baggage', 'about', 'his', 'relationship', 'with', 'his', 'father', ',', 'and', 'vice', 'versa', ',', 'but', 'that', 'hardly', 'accounts', 'for', 'him', 'becoming', 'a', 'heroin', '-', 'using', 'murdering', 'embezzler.<br', '/><br', '/>As', 'the', 'scheming', 'older', 'brother', ',', 'a', 'corpulent', 'Philip', 'Seymour', 'Hoffman', 'dominates', 'the', 'film', ',', 'but', 'he', 'is', 'well', 'supported', 'by', 'Ethan', 'Hawke', 'as', 'his', 'bullied', ',', 'inadequate', 'younger', 'brother', '.', 'Albert', 'Finney', 'as', 'their', 'father', 'seems', 'to', 'be', 'in', 'a', 'constant', 'state', 'of', 'rage', 'but', 'then', 'the', 'script', 'calls', 'for', 'that', '.', 'Marisa', 'Tomei', 'as', 'the', 'older', 'brother', \"'s\", 'cheating', 'wife', 'at', 'the', 'age', 'of', '42', 'puts', 'in', 'the', 'sexiest', 'performance', 'I', \"'ve\", 'seen', 'in', 'many', 'a', 'year', '.', 'The', 'film', 'literally', 'starts', 'with', 'a', 'bang', ',', 'but', 'we', 'are', 'out', 'of', 'that', 'comfort', 'zone', 'pretty', 'quickly.<br', '/><br', '/>I', 'do', \"n't\", 'know', 'the', 'origins', 'of', 'this', 'story', 'by', 'first', 'time', 'scriptwriter', 'Kelly', 'Masterton', 'but', 'I', 'suspect', 'that', 'like', 'Lumet', \"'s\", 'great', '70', \"'s\", 'film', '\"', 'Dog', 'Day', 'Afternoon', '\"', 'it', 'is', 'based', 'on', 'fact', '\\x96', 'it', \"'s\", 'too', 'silly', 'to', 'be', 'untrue', '.', 'Lumet', 'is', 'just', 'about', 'the', 'last', 'of', 'those', 'immensely', 'versatile', 'old', '-', 'time', 'craftsman', 'studio', 'directors', 'who', 'with', 'immense', 'speed', 'were', 'able', 'to', 'direct', 'just', 'about', 'anything', 'that', 'was', 'put', 'in', 'front', 'of', 'them', '.', 'Some', 'great', 'films', 'were', 'produced', 'that', 'way', 'as', 'well', 'as', 'some', 'classic', 'turkeys', '.', 'This', 'is', \"n't\", 'a', 'classic', 'of', 'either', 'sort', '\\x96', 'it', \"'s\", 'a', 'well', '-', 'crafted', 'piece', 'of', 'downbeat', 'entertainment', '.', 'It', 'will', 'probably', 'leave', 'you', 'feeling', 'that', 'you', 'were', 'lucky', 'not', 'be', 'a', 'member', 'of', 'a', 'family', 'as', 'dysfunctional', 'as', 'this', 'one', ',', 'but', 'still', 'wondering', 'as', 'to', 'how', 'they', 'got', 'that', 'way', '.', 'We', 'do', 'know', 'the', 'parents', 'were', 'happy', 'but', 'we', 'see', 'so', 'little', 'of', 'the', 'mother', 'and', 'hear', 'so', 'little', 'about', 'her', 'it', 'is', 'impossible', 'pick', 'up', 'on', 'her', 'relationship', 'with', 'the', 'boys', '.', '(', 'There', 'is', 'also', 'a', 'daughter', 'whose', 'presence', 'seems', 'redundant', ')', '.', 'Well', ',', 'like', 'Tolstoy', ',', 'we', 'have', 'to', 'conclude', 'that', '\"', 'each', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '\"', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWffaWL3iRTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "#creating valdation set and setting random_state so that we get training and validation set same each time\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED),split_ratio=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id_yap8ZiwWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f47c664e-1c6a-438c-c115-fd02ee1d41f5"
      },
      "source": [
        "# Checking length of train,test, validation splits\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPuOWRL9i7iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building the vocabulary, only keeping the most common max_size tokens using our training set\n",
        "MAX_VOCAB_SIZE = 20000 \n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBB-J4F3jjYH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b55d1c6a-f536-44f3-878c-1eb93da8674d"
      },
      "source": [
        "# Unique tokens in TEXT vocabulary and LABEL vocabulary\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 20002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyBqTqgpkSuI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fde626e2-cc10-4616-a610-111934759cd0"
      },
      "source": [
        "#to view the most common words in the vocabulary and their frequencies\n",
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 231896), (',', 220268), ('.', 189990), ('and', 125248), ('a', 125148), ('of', 115090), ('to', 107050), ('is', 87499), ('in', 70003), ('I', 62195), ('it', 61349), ('that', 56156), ('\"', 50662), (\"'s\", 49440), ('this', 48213), ('-', 42289), ('/><br', 40628), ('was', 40052), ('as', 35049), ('with', 34036)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFk-vs5tkaIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5304512-e886-425e-97e4-a1771067ab6d"
      },
      "source": [
        "# We can also see the vocabulary directly using either the stoi (string to int) or itos (int to string) method.\n",
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgU55WZxkn02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5d3ceaf-ff55-4cbf-8bcf-620a199601f8"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f8eefc152f0>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOaqYe_7kqQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We then create the iterators. We iterate over these in the training/evaluation loop, and \n",
        "#they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
        "#We'll use a BucketIterator which is a special type of iterator that will return\n",
        "#a batch of examples where each example is of a similar length, minimizing \n",
        "#the amount of padding per example.\n",
        "#We also want to place the tensors returned by the iterator on the \n",
        "#GPU (if you're using one). PyTorch handles this using torch.device, \n",
        "#we then pass this device to the iterator.\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,sort_within_batch=True, #for packed_padded_sequences\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymh4eoUArlkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "9300dc3e-b46b-4ff0-9105-6cc7ddb78499"
      },
      "source": [
        "#testing the iterators \n",
        "#number of rows depends on the longest document in the respective batch\n",
        "print('Train')\n",
        "for batch in train_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([509, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([60, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([42, 128])\n",
            "Target vector size: torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Gk4EtKlKg-",
        "colab_type": "text"
      },
      "source": [
        "Building our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXHY5PyFlIAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn # neural network library of pytorch\n",
        "# Defining our RNN class as a sub-class of nn.Module \n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=NUM_LAYERS,\n",
        "                           bidirectional=BIDIRECTIONAL)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_length):\n",
        "\n",
        "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        \n",
        "        # combine both directions\n",
        "        combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        \n",
        "        return self.fc(combined.squeeze(0)).view(-1)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LUNbQUWnxGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to create an instance of our RNN class by passing INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM and OUTPUT_DIM\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 2\n",
        "OUTPUT_DIM = 1\n",
        "BIDIRECTIONAL=True\n",
        "\n",
        "model = LSTM(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTD5-cWOoBBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56fbb144-ed2f-4660-9d6a-bdfc8e3ca878"
      },
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"tell us how many trainable parameters our model has so we can compare the number of parameters across different models\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 3,219,969 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0GQrknIrzyg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "049e2ac7-3a2b-46f9-f516-ea224ce9d42e"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM(\n",
            "  (embedding): Embedding(20002, 128)\n",
            "  (rnn): LSTM(128, 128, num_layers=2, bidirectional=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au9l-qJuoNCA",
        "colab_type": "text"
      },
      "source": [
        "Training our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iirzXMOeoLWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch.optim as optim #optimizer\n",
        "# we use ADAM with learning rate as 0.0001\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFGu_0Gko8iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#putting model and criterion to our device\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4xUsPWFpGd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_binary_accuracy(model, data_iterator, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(data_iterator):\n",
        "            text, text_lengths = batch_data.text\n",
        "            logits = model(text, text_lengths)\n",
        "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
        "            num_examples += batch_data.label.size(0)\n",
        "            correct_pred += (predicted_labels == batch_data.label.long()).sum()\n",
        "        return correct_pred.float()/num_examples * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGZYuWns0AMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a31f21c4-e3e0-43e4-fd08-e4d7d6b34659"
      },
      "source": [
        "import time \n",
        "start_time = time.time()\n",
        "NUM_EPOCHS=15\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for batch_idx, batch_data in enumerate(train_iterator):\n",
        "        text,text_lengths = batch_data.text\n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(text,text_lengths)\n",
        "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.label)\n",
        "        #setting gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_iterator):03d} | '\n",
        "                   f'Cost: {cost:.4f}')\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{compute_binary_accuracy(model, train_iterator, device):.2f}%'\n",
        "              f'\\nvalid accuracy: '\n",
        "              f'{compute_binary_accuracy(model, valid_iterator, device):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001/015 | Batch 000/157 | Cost: 0.6904\n",
            "Epoch: 001/015 | Batch 050/157 | Cost: 0.6881\n",
            "Epoch: 001/015 | Batch 100/157 | Cost: 0.6871\n",
            "Epoch: 001/015 | Batch 150/157 | Cost: 0.6871\n",
            "training accuracy: 59.45%\n",
            "valid accuracy: 59.04%\n",
            "Time elapsed: 0.30 min\n",
            "Epoch: 002/015 | Batch 000/157 | Cost: 0.6632\n",
            "Epoch: 002/015 | Batch 050/157 | Cost: 0.6572\n",
            "Epoch: 002/015 | Batch 100/157 | Cost: 0.6055\n",
            "Epoch: 002/015 | Batch 150/157 | Cost: 0.5178\n",
            "training accuracy: 73.04%\n",
            "valid accuracy: 72.52%\n",
            "Time elapsed: 0.59 min\n",
            "Epoch: 003/015 | Batch 000/157 | Cost: 0.5450\n",
            "Epoch: 003/015 | Batch 050/157 | Cost: 0.5334\n",
            "Epoch: 003/015 | Batch 100/157 | Cost: 0.4344\n",
            "Epoch: 003/015 | Batch 150/157 | Cost: 0.5122\n",
            "training accuracy: 77.92%\n",
            "valid accuracy: 75.84%\n",
            "Time elapsed: 0.89 min\n",
            "Epoch: 004/015 | Batch 000/157 | Cost: 0.4082\n",
            "Epoch: 004/015 | Batch 050/157 | Cost: 0.4937\n",
            "Epoch: 004/015 | Batch 100/157 | Cost: 0.4615\n",
            "Epoch: 004/015 | Batch 150/157 | Cost: 0.4379\n",
            "training accuracy: 81.31%\n",
            "valid accuracy: 78.82%\n",
            "Time elapsed: 1.19 min\n",
            "Epoch: 005/015 | Batch 000/157 | Cost: 0.4424\n",
            "Epoch: 005/015 | Batch 050/157 | Cost: 0.3573\n",
            "Epoch: 005/015 | Batch 100/157 | Cost: 0.5782\n",
            "Epoch: 005/015 | Batch 150/157 | Cost: 0.3876\n",
            "training accuracy: 82.80%\n",
            "valid accuracy: 78.92%\n",
            "Time elapsed: 1.49 min\n",
            "Epoch: 006/015 | Batch 000/157 | Cost: 0.3820\n",
            "Epoch: 006/015 | Batch 050/157 | Cost: 0.3497\n",
            "Epoch: 006/015 | Batch 100/157 | Cost: 0.4060\n",
            "Epoch: 006/015 | Batch 150/157 | Cost: 0.3731\n",
            "training accuracy: 85.24%\n",
            "valid accuracy: 81.22%\n",
            "Time elapsed: 1.78 min\n",
            "Epoch: 007/015 | Batch 000/157 | Cost: 0.3295\n",
            "Epoch: 007/015 | Batch 050/157 | Cost: 0.3845\n",
            "Epoch: 007/015 | Batch 100/157 | Cost: 0.3767\n",
            "Epoch: 007/015 | Batch 150/157 | Cost: 0.3982\n",
            "training accuracy: 85.94%\n",
            "valid accuracy: 81.80%\n",
            "Time elapsed: 2.08 min\n",
            "Epoch: 008/015 | Batch 000/157 | Cost: 0.4055\n",
            "Epoch: 008/015 | Batch 050/157 | Cost: 0.4594\n",
            "Epoch: 008/015 | Batch 100/157 | Cost: 0.2931\n",
            "Epoch: 008/015 | Batch 150/157 | Cost: 0.3166\n",
            "training accuracy: 84.25%\n",
            "valid accuracy: 81.00%\n",
            "Time elapsed: 2.37 min\n",
            "Epoch: 009/015 | Batch 000/157 | Cost: 0.3202\n",
            "Epoch: 009/015 | Batch 050/157 | Cost: 0.2402\n",
            "Epoch: 009/015 | Batch 100/157 | Cost: 0.2931\n",
            "Epoch: 009/015 | Batch 150/157 | Cost: 0.2524\n",
            "training accuracy: 88.60%\n",
            "valid accuracy: 82.98%\n",
            "Time elapsed: 2.67 min\n",
            "Epoch: 010/015 | Batch 000/157 | Cost: 0.2400\n",
            "Epoch: 010/015 | Batch 050/157 | Cost: 0.2969\n",
            "Epoch: 010/015 | Batch 100/157 | Cost: 0.2732\n",
            "Epoch: 010/015 | Batch 150/157 | Cost: 0.2065\n",
            "training accuracy: 89.67%\n",
            "valid accuracy: 84.08%\n",
            "Time elapsed: 2.97 min\n",
            "Epoch: 011/015 | Batch 000/157 | Cost: 0.2107\n",
            "Epoch: 011/015 | Batch 050/157 | Cost: 0.1935\n",
            "Epoch: 011/015 | Batch 100/157 | Cost: 0.2612\n",
            "Epoch: 011/015 | Batch 150/157 | Cost: 0.1922\n",
            "training accuracy: 90.11%\n",
            "valid accuracy: 84.44%\n",
            "Time elapsed: 3.27 min\n",
            "Epoch: 012/015 | Batch 000/157 | Cost: 0.2777\n",
            "Epoch: 012/015 | Batch 050/157 | Cost: 0.2528\n",
            "Epoch: 012/015 | Batch 100/157 | Cost: 0.1970\n",
            "Epoch: 012/015 | Batch 150/157 | Cost: 0.2262\n",
            "training accuracy: 91.74%\n",
            "valid accuracy: 85.24%\n",
            "Time elapsed: 3.56 min\n",
            "Epoch: 013/015 | Batch 000/157 | Cost: 0.2035\n",
            "Epoch: 013/015 | Batch 050/157 | Cost: 0.1933\n",
            "Epoch: 013/015 | Batch 100/157 | Cost: 0.2338\n",
            "Epoch: 013/015 | Batch 150/157 | Cost: 0.2009\n",
            "training accuracy: 92.34%\n",
            "valid accuracy: 85.32%\n",
            "Time elapsed: 3.85 min\n",
            "Epoch: 014/015 | Batch 000/157 | Cost: 0.1919\n",
            "Epoch: 014/015 | Batch 050/157 | Cost: 0.1573\n",
            "Epoch: 014/015 | Batch 100/157 | Cost: 0.0973\n",
            "Epoch: 014/015 | Batch 150/157 | Cost: 0.1930\n",
            "training accuracy: 89.52%\n",
            "valid accuracy: 83.42%\n",
            "Time elapsed: 4.14 min\n",
            "Epoch: 015/015 | Batch 000/157 | Cost: 0.1455\n",
            "Epoch: 015/015 | Batch 050/157 | Cost: 0.3531\n",
            "Epoch: 015/015 | Batch 100/157 | Cost: 0.1631\n",
            "Epoch: 015/015 | Batch 150/157 | Cost: 0.1334\n",
            "training accuracy: 94.03%\n",
            "valid accuracy: 86.48%\n",
            "Time elapsed: 4.44 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppDhqFXvblpk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a7a81924-2b8f-4637-caee-18a8a0cfec38"
      },
      "source": [
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "#prints the confusion matrix as well as test = True\n",
        "print(f'Test accuracy: {compute_binary_accuracy(model, test_iterator, device):.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Training Time: 7.50 min\n",
            "Test accuracy: 85.64%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoIMCsSQvva4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy \n",
        "nlp = spacy.load('en')\n",
        "def predict_sentiment(model, sentence):\n",
        "    \"\"\" inputs the sentence and the model and outputs the sentiment of sentence based on the model\"\"\"\n",
        "    model.eval()\n",
        "    #creating list of tokens for our sentence\n",
        "    tokenized = [token.text for token in nlp.tokenizer(sentence)]\n",
        "    # index of token in TEXT.vocab\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor,length_tensor))\n",
        "    if (prediction.item())>=0.5:\n",
        "        return \"Positive with score \"+ str(prediction.item())\n",
        "    else:\n",
        "        return \"Negative with score \"+ str(prediction.item())\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4hLReu0wxL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a62021b-5515-4a55-adf4-fe6e2ec0031a"
      },
      "source": [
        "predict_sentiment(model, \"I really love this movie.This movie is so great!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Positive with score 0.9901903867721558'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    }
  ]
}